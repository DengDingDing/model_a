import os
import torch
print(torch.cuda.is_available()) 
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import time, argparse
from PIL import Image
import numpy as np
from torchvision import transforms
from torchvision.utils import save_image as imwrite
from utils.utils import print_args, load_restore_ckpt, tensor_metric, load_restore_ckpt_with_optim

import clip
from model.Prompt import Prompts,TextEncoder
from utils.clip_score import L_clip_from_feature


# print(torch.cuda.is_available()) 
# print(torch.cuda.current_device())

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

CLIP_model, preprocess = clip.load("ViT-B/32", device = torch.device("cpu"), download_root="./clip_model/")#ViT-B/32
model_path = './prompt-ckpt/prompt_iter_12000_klrate.pth'
CLIP_model.load_state_dict(torch.load(model_path, map_location=device))
CLIP_model.to(device)
for para in CLIP_model.parameters():
    para.requires_grad = False

def getTextFeature():
    # return size = types*512
    length_prompt = 5
    #load clip
    prompt_path = './prompt-ckpt/prompt_iter_12000_klrate.pth'
    learn_prompt=Prompts(CLIP_model,prompt_path).cuda()
    learn_prompt =  torch.nn.DataParallel(learn_prompt)
    for name, param in learn_prompt.named_parameters():
        param.requires_grad_(False)
    text_encoder = TextEncoder(CLIP_model)
    embedding_prompt=learn_prompt.module.embedding_prompt
    tokenized_prompts= torch.cat([clip.tokenize(p) for p in [" ".join(["X"]*length_prompt)]])
    text_features = text_encoder(embedding_prompt,tokenized_prompts)
    
    return text_features

# types = ["clear","color","haze","dark",
#         "haze2dark","dark2haze",
#         "color2dark","dark2color"]
types = ["clear","color","haze","dark","noise",
        "haze2dark","dark2haze","haze2noise","dark2noise",
        "color2dark","dark2color","color2noise",
        "haze2dark2noise","dark2haze2noise",
        "color2dark2noise","dark2color2noise"]
# types=["label","water"]


def main(args):

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)
    #train
    print('> Model Initialization...')

    text_features = getTextFeature()
    restorer,_,_ = load_restore_ckpt_with_optim(device, freeze_model=True, ckpt_name=args.restore_model_path)

    # os.makedirs(args.output,exist_ok=True)
    test(restorer, text_features, device, types, 0)
    # psnr_t1, ssim_t1, psnr_t2, ssim_t2 = test(restorer,text_features,device,types,0)
    # print("[epoch %d] Test images PSNR1: %.4f SSIM1: %.4f"%(1, psnr_t1,ssim_t1))
    # print("[epoch %d] Test images PSNR2: %.4f SSIM2: %.4f"%(1, psnr_t2,ssim_t2))
    # files = os.listdir(argspar.input)
    # time_record = []
    # for i in files:
    #     lq = Image.open(f'{argspar.input}/{i}')

    #     with torch.no_grad():
    #         lq_re = torch.Tensor((np.array(lq)/255).transpose(2, 0, 1)).unsqueeze(0).to("cuda" if torch.cuda.is_available() else "cpu")
    #         lq_em = transform_resize(lq).unsqueeze(0).to("cuda" if torch.cuda.is_available() else "cpu")

    #         start_time = time.time()
            
    #         if args.prompt == None:
    #             L_clip_Feature = L_clip_from_feature()
    #             _, rate = L_clip_Feature(lq_re, text_features)
    #             idx = rate[0].argmax()
    #             print(f'This is {types[idx]} degradation estimated by visual embedder.')
    #         else:
    #             idx = types.index(args.prompt)
    #             print(f'This is {args.prompt} degradation generated by input text.')
            
    #         text_feature = text_features[idx,:]
    #         text_feature = text_feature.unsqueeze(0)
    #         out = restorer(lq_re, text_feature)

    #         run_time = time.time()-start_time
    #         time_record.append(run_time)

    #         if args.concat:
    #             out = torch.cat((lq_re, out), dim=3)

    #         imwrite(out, f'{args.output}/{i}', range=(0, 1))

    #         print(f'{i} Running Time: {run_time:.4f}.')
    # print(f'Average time is {np.mean(np.array(run_time))}')

transform_resize = transforms.Compose([
        transforms.Resize([960,540]),
        # transforms.ToTensor()
])
def test(restorer, text_features, device, types,epoch=-1):
    L_clip_Feature = L_clip_from_feature(CLIP_model)
    # combine_type = types
    test_input = "./test"
    # input_dir=["5244359-uhd_2562_1440_25fps"]
    # "3028395-uhd_3840_2160_24fps"
    # "12744733_2160_3840_30fps"
    # "2103040-hd_1920_1080_30fps", "5198908-uhd_4096_2160_25fps","5244359-uhd_2562_1440_25fps","5562986-uhd_3840_2160_24fps","7242694-hd_1920_1080_30fps","11008653-hd_3840_2160_30fps","12967834_3840_2160_24fps",

    input_dir = ["13022142_2160_3840_30fps"]

    # input_dir = ["13643105-uhd_3840_2160_24fps", "19077298-uhd_3840_2160_30fps","20356083-hd_1920_1080_25fps"]

    print(len(input_dir))
    for i in range(len(input_dir)):
        output = "./out/" + input_dir[i] + "/"
        os.makedirs(output,exist_ok=True)
        file_list =  os.listdir(f'{test_input}/{input_dir[i]}/')
        file_list.sort()
        for j in range(len(file_list)):
            # hq = Image.open(f'{test_input}/{combine_type[0]}/{file_list[j]}').convert("RGB")
            # hq = Image.open(f'{test_input}/{combine_type[i+1]}/{file_list[j]}').convert("RGB")
            lq = Image.open(f'{test_input}/{input_dir[i]}/{file_list[j]}').convert("RGB")
            restorer.eval()
            with torch.no_grad():
                lq = np.array(lq)
                lq = (lq - np.min(lq)) / (np.max(lq) - np.min(lq))
                lq_re = torch.Tensor((lq).transpose(2, 0, 1)).unsqueeze(0).to("cuda" if torch.cuda.is_available() else "cpu")
                # lq_re = torch.Tensor((np.array(lq)/255).transpose(2, 0, 1)).unsqueeze(0).to("cuda" if torch.cuda.is_available() else "cpu")
                # hq = torch.Tensor((np.array(hq)/255).transpose(2, 0, 1)).unsqueeze(0).to("cuda" if torch.cuda.is_available() else "cpu")

                lq_re = transform_resize(lq_re)
                # hq = transform_resize(hq)

                starttime = time.time()

                # idx1 = types.index(combine_type[i+1])
                # text_feature1 = text_features[idx1,:]
                # text_feature1 = text_feature1.unsqueeze(0)

                
                # _, rate = L_clip_Feature(lq_re, text_features)
                # idx2 = rate[0].argmax()
                # text_feature2 = text_features[idx2,:]
                # text_feature2 = text_feature2.unsqueeze(0)
                # out_1 = restorer(lq_re, text_feature1)
                # if idx1 != idx2:
                #     print(idx1,types[idx1],idx2,types[idx2])
                #     out_2 = restorer(lq_re, text_feature2)
                # else:
                #     out_2 = out_1
                
                _, rate = L_clip_Feature(lq_re, text_features)
                # print(rate)
                # rate = [[0, 0, 0, 1, 0]]
                
                l, _  = text_features.size()
                # print(l)
                print(file_list[j][:-4] +'.png',rate[0])
                f = torch.zeros(512).to(device)
                for k in range(l):
                    f = f + text_features[k,:] * rate[0][k]
                    # print(f[0])
                    # f = f + text_features[k,:] * rate_set[k]
                text_feature2 = f.unsqueeze(0)
                
                # out_1 = restorer(lq_re, text_feature1)
                out_2 = restorer(lq_re, text_feature2)
                
                # endtime1 = time.time()

                # imwrite(torch.cat((lq_re, out_2), dim=3), output \
                #     + file_list[j][:-4] + '_' + str(epoch) + '_' + combine_type[i+1] + '.png', range=(0, 1))
                # imwrite(torch.cat((lq_re, out_2, hq), dim=3), output \
                #     + file_list[j][:-4] + '_' + str(epoch) + '_' + combine_type[i+1] + '.png', range=(0, 1))
                imwrite(out_2, output + file_list[j][:-4] +'.png', range=(0, 1))
                # imwrite(out_2, output + "/out/" \
                #     + file_list[j][:-4] + '_' + str(epoch) + '_' + combine_type[i+1] + '.png', range=(0, 1))
            # psnr_1 += tensor_metric(hq, out_1, 'PSNR', data_range=1)
            # ssim_1 += tensor_metric(hq, out_1, 'SSIM', data_range=1)
            # psnr = tensor_metric(hq, out_2, 'PSNR', data_range=1)
            # ssim = tensor_metric(hq, out_2, 'SSIM', data_range=1)
            # psnr_2 += tensor_metric(hq, out_2, 'PSNR', data_range=1)
            # ssim_2 += tensor_metric(hq, out_2, 'SSIM', data_range=1)
            # print('The ' + file_list[j][:-4] + ' '+ str(psnr) + ' ' + str(ssim)+' Time:' + str(endtime1 - starttime) + 's.')
            # print('The {:<4} {:.4f} {:.4f} Time: {:.4f}s.'.format(file_list[j][:-4], psnr, ssim, endtime1 - starttime))

    # length = len(combine_type) - 1
    # return psnr_1 / (len(file_list)*length), ssim_1 / (len(file_list)*length),\
    #     psnr_2 / (len(file_list)*length), ssim_2 / (len(file_list)*length)


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description = "OneRestore Running")

    parser.add_argument("--restore-model-path", type=str, default = "./ckpts/OneRestore_model_161_2080.tar", help = 'restore model path')
    # parser.add_argument("--restore-model-path", type=str, default = "./ckpts/OneRestore_model_181_4090.tar", help = 'restore model path')
    # load model

    # select model automatic (prompt=False) or manual (prompt=True, text={'clear', 'low', 'haze', 'rain', 'snow',\
    #                'low_haze', 'low_rain', 'low_snow', 'haze_rain', 'haze_snow', 'low_haze_rain', 'low_haze_snow'})
    parser.add_argument("--prompt", type=str, default = None, help = 'prompt')

    parser.add_argument("--input", type=str, default = "./image/", help = 'image path')
    parser.add_argument("--concat", action='store_true', help = 'output path')

    argspar = parser.parse_args()

    print_args(argspar)

    main(argspar)
